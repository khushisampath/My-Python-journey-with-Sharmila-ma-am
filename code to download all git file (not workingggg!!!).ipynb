{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aff4e510-2276-442a-990d-6b13b22b6fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from: https://github.com/khushisampath/My-Python-journey-with-Sharmila-ma-am\n",
      "Files will be saved to: github_downloads\n",
      "--------------------------------------------------\n",
      "Discovering files in repository...\n",
      "No files found to download!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import threading\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class GitHubDownloader:\n",
    "    def __init__(self, repo_url, max_workers=5):\n",
    "        self.repo_url = repo_url\n",
    "        self.base_url = repo_url.replace('github.com', 'raw.githubusercontent.com').replace('/tree/', '/') + '/'\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        self.download_dir = \"github_downloads\"\n",
    "        self.max_workers = max_workers\n",
    "        self.lock = threading.Lock()\n",
    "        self.downloaded_files = 0\n",
    "        \n",
    "        # Create download directory\n",
    "        os.makedirs(self.download_dir, exist_ok=True)\n",
    "    \n",
    "    def get_file_urls(self):\n",
    "        \"\"\"Get all raw file URLs from the GitHub repository\"\"\"\n",
    "        print(\"Discovering files in repository...\")\n",
    "        \n",
    "        file_urls = []\n",
    "        \n",
    "        def crawl_directory(url, path=\"\"):\n",
    "            try:\n",
    "                response = self.session.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Find all file and directory links\n",
    "                for link in soup.find_all('a', {'class': 'js-navigation-open'}):\n",
    "                    href = link.get('href')\n",
    "                    name = link.text.strip()\n",
    "                    \n",
    "                    if href and '/tree/' in href and name not in ['.', '..']:\n",
    "                        # It's a directory\n",
    "                        dir_url = urljoin('https://github.com', href)\n",
    "                        crawl_directory(dir_url, os.path.join(path, name))\n",
    "                    elif href and '/blob/' in href:\n",
    "                        # It's a file\n",
    "                        raw_url = href.replace('/blob/', '/')  # Convert to raw URL\n",
    "                        raw_url = urljoin('https://raw.githubusercontent.com', raw_url)\n",
    "                        file_path = os.path.join(path, name)\n",
    "                        file_urls.append((raw_url, file_path))\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error crawling {url}: {e}\")\n",
    "        \n",
    "        # Start crawling from the main repository URL\n",
    "        crawl_directory(self.repo_url)\n",
    "        return file_urls\n",
    "    \n",
    "    def download_file(self, url, file_path):\n",
    "        \"\"\"Download a single file\"\"\"\n",
    "        try:\n",
    "            # Create directory structure if needed\n",
    "            full_path = os.path.join(self.download_dir, file_path)\n",
    "            os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "            \n",
    "            # Download the file\n",
    "            response = self.session.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Save the file\n",
    "            with open(full_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            \n",
    "            # Update progress safely\n",
    "            with self.lock:\n",
    "                self.downloaded_files += 1\n",
    "                print(f\"✓ Downloaded ({self.downloaded_files}): {file_path}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"✗ Failed to download {file_path}: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error with {file_path}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def download_all_files(self):\n",
    "        \"\"\"Download all files using multithreading\"\"\"\n",
    "        print(f\"Starting download from: {self.repo_url}\")\n",
    "        print(f\"Files will be saved to: {self.download_dir}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Get all file URLs\n",
    "        file_urls = self.get_file_urls()\n",
    "        \n",
    "        if not file_urls:\n",
    "            print(\"No files found to download!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(file_urls)} files to download\")\n",
    "        print(\"Starting download with multithreading...\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use ThreadPoolExecutor for efficient multithreading\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            # Submit all download tasks\n",
    "            future_to_file = {\n",
    "                executor.submit(self.download_file, url, file_path): (url, file_path)\n",
    "                for url, file_path in file_urls\n",
    "            }\n",
    "            \n",
    "            # Process completed downloads\n",
    "            successful = 0\n",
    "            failed = 0\n",
    "            \n",
    "            for future in as_completed(future_to_file):\n",
    "                url, file_path = future_to_file[future]\n",
    "                try:\n",
    "                    if future.result():\n",
    "                        successful += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Exception for {file_path}: {e}\")\n",
    "                    failed += 1\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"-\" * 50)\n",
    "        print(\"Download Summary:\")\n",
    "        print(f\"Total files: {len(file_urls)}\")\n",
    "        print(f\"Successful: {successful}\")\n",
    "        print(f\"Failed: {failed}\")\n",
    "        print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"Download location: {os.path.abspath(self.download_dir)}\")\n",
    "\n",
    "# Alternative simpler version without directory crawling\n",
    "class SimpleGitHubDownloader:\n",
    "    def __init__(self, repo_url, max_workers=5):\n",
    "        self.repo_url = repo_url\n",
    "        self.base_api_url = f\"https://api.github.com/repos/{repo_url.split('github.com/')[1]}/contents\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            'Accept': 'application/vnd.github.v3+json'\n",
    "        })\n",
    "        self.download_dir = \"github_downloads\"\n",
    "        self.max_workers = max_workers\n",
    "        self.lock = threading.Lock()\n",
    "        self.downloaded_files = 0\n",
    "        \n",
    "        os.makedirs(self.download_dir, exist_ok=True)\n",
    "    \n",
    "    def get_files_via_api(self):\n",
    "        \"\"\"Get files using GitHub API (simpler but might have rate limits)\"\"\"\n",
    "        print(\"Fetching repository contents via GitHub API...\")\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(self.base_api_url)\n",
    "            response.raise_for_status()\n",
    "            contents = response.json()\n",
    "            \n",
    "            file_urls = []\n",
    "            \n",
    "            for item in contents:\n",
    "                if item['type'] == 'file':\n",
    "                    file_urls.append((item['download_url'], item['path']))\n",
    "                elif item['type'] == 'dir':\n",
    "                    # For simplicity, we'll just get files from root directory\n",
    "                    pass\n",
    "            \n",
    "            return file_urls\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error using GitHub API: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def download_all_files_simple(self):\n",
    "        \"\"\"Simplified download using GitHub API\"\"\"\n",
    "        file_urls = self.get_files_via_api()\n",
    "        \n",
    "        if not file_urls:\n",
    "            print(\"No files found via API. Trying alternative method...\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Found {len(file_urls)} files to download\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            # Submit download tasks\n",
    "            futures = [executor.submit(self.download_file, url, path) \n",
    "                      for url, path in file_urls]\n",
    "            \n",
    "            successful = sum(1 for future in futures if future.result())\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(f\"\\nDownloaded {successful}/{len(file_urls)} files in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "def main():\n",
    "    # Repository URL\n",
    "    repo_url = \"https://github.com/khushisampath/My-Python-journey-with-Sharmila-ma-am\"\n",
    "    \n",
    "    # Create downloader instance\n",
    "    downloader = GitHubDownloader(repo_url, max_workers=8)\n",
    "    \n",
    "    try:\n",
    "        # Download all files\n",
    "        downloader.download_all_files()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nDownload interrupted by user!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0696ca08-4e78-4ba7-a631-affeb919abf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\femal\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\femal\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: requests in c:\\users\\femal\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\femal\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c608c9d-765b-4b17-b94e-37ba8ecaa89b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2442328538.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[4], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    python github_downloader.py\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python github_downloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95bb5ea2-c57a-428d-8be5-72a8c49a5887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download from: https://github.com/khushisampath/My-Python-journey-with-Sharmila-ma-am/blob/main/\n",
      "Files will be saved to: C:\\Users\\femal\\My python journey\\Sharmila\\github_python_journey\n",
      "------------------------------------------------------------\n",
      "Discovering files in directory: https://github.com/khushisampath/My-Python-journey-with-Sharmila-ma-am/tree/main/\n",
      "No files found in the directory!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import threading\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "class GitHubDirectoryDownloader:\n",
    "    def __init__(self, directory_url, max_workers=8):\n",
    "        self.directory_url = directory_url\n",
    "        # Convert to raw content URL\n",
    "        self.raw_base_url = directory_url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')\n",
    "        # For web scraping\n",
    "        self.github_base_url = directory_url.replace('/blob/', '/tree/')\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        self.download_dir = \"github_python_journey\"\n",
    "        self.max_workers = max_workers\n",
    "        self.lock = threading.Lock()\n",
    "        self.downloaded_files = 0\n",
    "        self.total_files = 0\n",
    "        \n",
    "        # Create download directory\n",
    "        os.makedirs(self.download_dir, exist_ok=True)\n",
    "    \n",
    "    def get_files_from_directory(self):\n",
    "        \"\"\"Get all file URLs from the specific GitHub directory\"\"\"\n",
    "        print(f\"Discovering files in directory: {self.github_base_url}\")\n",
    "        \n",
    "        file_urls = []\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(self.github_base_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find all file links in the directory\n",
    "            file_links = soup.find_all('a', {\n",
    "                'class': 'js-navigation-open', \n",
    "                'href': True\n",
    "            })\n",
    "            \n",
    "            for link in file_links:\n",
    "                href = link.get('href')\n",
    "                filename = link.text.strip()\n",
    "                \n",
    "                # Skip directories and special files\n",
    "                if (href and '/blob/' in href and \n",
    "                    not filename.startswith('.') and \n",
    "                    filename not in ['', '..', '.']):\n",
    "                    \n",
    "                    # Convert to raw GitHub URL\n",
    "                    raw_url = href.replace('/blob/', '/')\n",
    "                    raw_url = urljoin('https://raw.githubusercontent.com', raw_url)\n",
    "                    \n",
    "                    file_urls.append((raw_url, filename))\n",
    "                    print(f\"Found: {filename}\")\n",
    "            \n",
    "            return file_urls\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scanning directory: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def download_file(self, url, filename):\n",
    "        \"\"\"Download a single file with progress tracking\"\"\"\n",
    "        try:\n",
    "            full_path = os.path.join(self.download_dir, filename)\n",
    "            \n",
    "            # Download the file\n",
    "            response = self.session.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Get file size for progress tracking\n",
    "            file_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            # Save the file\n",
    "            with open(full_path, 'wb') as f:\n",
    "                if file_size == 0:\n",
    "                    f.write(response.content)\n",
    "                else:\n",
    "                    downloaded = 0\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                            downloaded += len(chunk)\n",
    "            \n",
    "            # Update progress safely\n",
    "            with self.lock:\n",
    "                self.downloaded_files += 1\n",
    "                print(f\"✓ [{self.downloaded_files}/{self.total_files}] Downloaded: {filename}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"✗ Failed to download {filename}: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error with {filename}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def download_all_files(self):\n",
    "        \"\"\"Download all files from the directory using multithreading\"\"\"\n",
    "        print(f\"Starting download from: {self.directory_url}\")\n",
    "        print(f\"Files will be saved to: {os.path.abspath(self.download_dir)}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Get all file URLs from the directory\n",
    "        file_urls = self.get_files_from_directory()\n",
    "        \n",
    "        if not file_urls:\n",
    "            print(\"No files found in the directory!\")\n",
    "            return\n",
    "        \n",
    "        self.total_files = len(file_urls)\n",
    "        print(f\"\\nFound {self.total_files} files to download\")\n",
    "        print(\"Starting multithreaded download...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use ThreadPoolExecutor for efficient multithreading\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            # Submit all download tasks\n",
    "            future_to_file = {\n",
    "                executor.submit(self.download_file, url, filename): (url, filename)\n",
    "                for url, filename in file_urls\n",
    "            }\n",
    "            \n",
    "            # Process completed downloads\n",
    "            successful = 0\n",
    "            failed = 0\n",
    "            \n",
    "            for future in as_completed(future_to_file):\n",
    "                url, filename = future_to_file[future]\n",
    "                try:\n",
    "                    if future.result():\n",
    "                        successful += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"✗ Exception for {filename}: {e}\")\n",
    "                    failed += 1\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"-\" * 60)\n",
    "        print(\"DOWNLOAD SUMMARY:\")\n",
    "        print(f\"Directory: {self.directory_url}\")\n",
    "        print(f\"Total files available: {self.total_files}\")\n",
    "        print(f\"Successfully downloaded: {successful}\")\n",
    "        print(f\"Failed downloads: {failed}\")\n",
    "        print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n",
    "        print(f\"Download location: {os.path.abspath(self.download_dir)}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "def main():\n",
    "    # Specific directory URL you provided\n",
    "    directory_url = \"https://github.com/khushisampath/My-Python-journey-with-Sharmila-ma-am/blob/main/\"\n",
    "    \n",
    "    # Create downloader instance with more workers for faster download\n",
    "    downloader = GitHubDirectoryDownloader(directory_url, max_workers=10)\n",
    "    \n",
    "    try:\n",
    "        # Download all files from the directory\n",
    "        downloader.download_all_files()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nDownload interrupted by user!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96aaa405-e96a-4fb6-8446-49fa3b859d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\femal\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\femal\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\femal\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1>📥 GitHub Directory Downloader</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p>Download all files from a GitHub directory using multithreading</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a3f25617404d719b3d25b0ad55cba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='https://github.com/khushisampath/My-Python-journey-with-Sharmila-ma-am/blob/main/',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5742d15ce254b9c8a6552bcada787fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='📁 Show Downloaded Files', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430fd138cdff4a3a978e9692ce2b67e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='warning', description='🗑️ Cleanup Downloads', style=ButtonStyle(), tooltip='Delete all do…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bd45eb5bdf4bc3bbee50fae5fffdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # GitHub Directory Downloader\n",
    "# \n",
    "# This notebook downloads all files from a specific GitHub directory using multithreading for faster downloads.\n",
    "# \n",
    "\n",
    "# %%\n",
    "# Install required packages (run this cell first)\n",
    "!pip install requests beautifulsoup4\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import requests\n",
    "import threading\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# %%\n",
    "class GitHubDirectoryDownloader:\n",
    "    def __init__(self, directory_url, max_workers=8):\n",
    "        self.directory_url = directory_url\n",
    "        # Convert to raw content URL\n",
    "        self.raw_base_url = directory_url.replace('github.com', 'raw.githubusercontent.com').replace('/blob/', '/')\n",
    "        # For web scraping\n",
    "        self.github_base_url = directory_url.replace('/blob/', '/tree/')\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        self.download_dir = \"github_python_journey\"\n",
    "        self.max_workers = max_workers\n",
    "        self.lock = threading.Lock()\n",
    "        self.downloaded_files = 0\n",
    "        self.total_files = 0\n",
    "        \n",
    "        # Create download directory\n",
    "        os.makedirs(self.download_dir, exist_ok=True)\n",
    "    \n",
    "    def get_files_from_directory(self):\n",
    "        \"\"\"Get all file URLs from the specific GitHub directory\"\"\"\n",
    "        display(HTML(f\"<b>Discovering files in directory:</b> {self.github_base_url}\"))\n",
    "        \n",
    "        file_urls = []\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(self.github_base_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find all file links in the directory\n",
    "            file_links = soup.find_all('a', {\n",
    "                'class': 'js-navigation-open', \n",
    "                'href': True\n",
    "            })\n",
    "            \n",
    "            for link in file_links:\n",
    "                href = link.get('href')\n",
    "                filename = link.text.strip()\n",
    "                \n",
    "                # Skip directories and special files\n",
    "                if (href and '/blob/' in href and \n",
    "                    not filename.startswith('.') and \n",
    "                    filename not in ['', '..', '.']):\n",
    "                    \n",
    "                    # Convert to raw GitHub URL\n",
    "                    raw_url = href.replace('/blob/', '/')\n",
    "                    raw_url = urljoin('https://raw.githubusercontent.com', raw_url)\n",
    "                    \n",
    "                    file_urls.append((raw_url, filename))\n",
    "                    display(HTML(f\"🔍 Found: <code>{filename}</code>\"))\n",
    "            \n",
    "            return file_urls\n",
    "            \n",
    "        except Exception as e:\n",
    "            display(HTML(f\"<span style='color: red;'>Error scanning directory: {e}</span>\"))\n",
    "            return []\n",
    "    \n",
    "    def download_file(self, url, filename):\n",
    "        \"\"\"Download a single file with progress tracking\"\"\"\n",
    "        try:\n",
    "            full_path = os.path.join(self.download_dir, filename)\n",
    "            \n",
    "            # Download the file\n",
    "            response = self.session.get(url, stream=True, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Get file size for progress tracking\n",
    "            file_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            # Save the file\n",
    "            with open(full_path, 'wb') as f:\n",
    "                if file_size == 0:\n",
    "                    f.write(response.content)\n",
    "                else:\n",
    "                    downloaded = 0\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                            downloaded += len(chunk)\n",
    "            \n",
    "            # Update progress safely\n",
    "            with self.lock:\n",
    "                self.downloaded_files += 1\n",
    "                progress = f\"[{self.downloaded_files}/{self.total_files}]\"\n",
    "                display(HTML(f\"✅ <span style='color: green;'>{progress}</span> Downloaded: <code>{filename}</code>\"))\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            display(HTML(f\"❌ <span style='color: red;'>Failed to download {filename}: {e}</span>\"))\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            display(HTML(f\"❌ <span style='color: red;'>Error with {filename}: {e}</span>\"))\n",
    "            return False\n",
    "    \n",
    "    def download_all_files(self):\n",
    "        \"\"\"Download all files from the directory using multithreading\"\"\"\n",
    "        display(HTML(f\"<h3>🚀 Starting Download</h3>\"))\n",
    "        display(HTML(f\"<b>Source:</b> {self.directory_url}\"))\n",
    "        display(HTML(f\"<b>Destination:</b> {os.path.abspath(self.download_dir)}\"))\n",
    "        display(HTML(\"<hr>\"))\n",
    "        \n",
    "        # Get all file URLs from the directory\n",
    "        file_urls = self.get_files_from_directory()\n",
    "        \n",
    "        if not file_urls:\n",
    "            display(HTML(\"<span style='color: orange;'>No files found in the directory!</span>\"))\n",
    "            return\n",
    "        \n",
    "        self.total_files = len(file_urls)\n",
    "        display(HTML(f\"<b>📊 Found {self.total_files} files to download</b>\"))\n",
    "        display(HTML(\"<b>⏳ Starting multithreaded download...</b>\"))\n",
    "        display(HTML(\"<hr>\"))\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use ThreadPoolExecutor for efficient multithreading\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            # Submit all download tasks\n",
    "            future_to_file = {\n",
    "                executor.submit(self.download_file, url, filename): (url, filename)\n",
    "                for url, filename in file_urls\n",
    "            }\n",
    "            \n",
    "            # Process completed downloads\n",
    "            successful = 0\n",
    "            failed = 0\n",
    "            \n",
    "            for future in as_completed(future_to_file):\n",
    "                url, filename = future_to_file[future]\n",
    "                try:\n",
    "                    if future.result():\n",
    "                        successful += 1\n",
    "                    else:\n",
    "                        failed += 1\n",
    "                except Exception as e:\n",
    "                    display(HTML(f\"❌ <span style='color: red;'>Exception for {filename}: {e}</span>\"))\n",
    "                    failed += 1\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Print summary\n",
    "        display(HTML(\"<hr>\"))\n",
    "        display(HTML(\"<h3>📋 DOWNLOAD SUMMARY</h3>\"))\n",
    "        display(HTML(f\"<b>Directory:</b> {self.directory_url}\"))\n",
    "        display(HTML(f\"<b>Total files available:</b> {self.total_files}\"))\n",
    "        display(HTML(f\"<b style='color: green;'>Successfully downloaded:</b> {successful}\"))\n",
    "        display(HTML(f\"<b style='color: red;'>Failed downloads:</b> {failed}\"))\n",
    "        display(HTML(f\"<b>Time taken:</b> {end_time - start_time:.2f} seconds\"))\n",
    "        display(HTML(f\"<b>Download location:</b> {os.path.abspath(self.download_dir)}\"))\n",
    "        \n",
    "        # Show downloaded files\n",
    "        if successful > 0:\n",
    "            display(HTML(\"<h4>📁 Downloaded Files:</h4>\"))\n",
    "            downloaded_files = os.listdir(self.download_dir)\n",
    "            for file in downloaded_files:\n",
    "                file_path = os.path.join(self.download_dir, file)\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                display(HTML(f\"• <code>{file}</code> ({file_size} bytes)\"))\n",
    "\n",
    "# %%\n",
    "# Create interactive widgets for the downloader\n",
    "def create_downloader_ui():\n",
    "    \"\"\"Create an interactive UI for the downloader\"\"\"\n",
    "    \n",
    "    # Widgets\n",
    "    url_input = widgets.Text(\n",
    "        value='https://github.com/khushisampath/My-Python-journey-with-Sharmila-ma-am/blob/main/',\n",
    "        placeholder='Enter GitHub directory URL',\n",
    "        description='GitHub URL:',\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "    \n",
    "    workers_slider = widgets.IntSlider(\n",
    "        value=8,\n",
    "        min=1,\n",
    "        max=20,\n",
    "        step=1,\n",
    "        description='Threads:',\n",
    "        continuous_update=False\n",
    "    )\n",
    "    \n",
    "    download_button = widgets.Button(\n",
    "        description='🚀 Start Download',\n",
    "        button_style='success',\n",
    "        tooltip='Click to start downloading'\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def on_download_click(b):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            try:\n",
    "                downloader = GitHubDirectoryDownloader(\n",
    "                    url_input.value, \n",
    "                    max_workers=workers_slider.value\n",
    "                )\n",
    "                downloader.download_all_files()\n",
    "            except Exception as e:\n",
    "                display(HTML(f\"<span style='color: red;'>Error: {e}</span>\"))\n",
    "    \n",
    "    download_button.on_click(on_download_click)\n",
    "    \n",
    "    # Display UI\n",
    "    display(HTML(\"<h1>📥 GitHub Directory Downloader</h1>\"))\n",
    "    display(HTML(\"<p>Download all files from a GitHub directory using multithreading</p>\"))\n",
    "    \n",
    "    display(widgets.VBox([\n",
    "        url_input,\n",
    "        workers_slider,\n",
    "        download_button,\n",
    "        output\n",
    "    ]))\n",
    "\n",
    "# %%\n",
    "# Run the interactive UI\n",
    "create_downloader_ui()\n",
    "\n",
    "# %%\n",
    "# Alternative: Direct download without UI (uncomment to use)\n",
    "# directory_url = \"https://github.com/khushisampath/My-Python-journey-with-Sharmila-ma-am/blob/main/\"\n",
    "# downloader = GitHubDirectoryDownloader(directory_url, max_workers=10)\n",
    "# downloader.download_all_files()\n",
    "\n",
    "# %%\n",
    "# Check downloaded files\n",
    "def show_downloaded_files():\n",
    "    download_dir = \"github_python_journey\"\n",
    "    if os.path.exists(download_dir):\n",
    "        files = os.listdir(download_dir)\n",
    "        if files:\n",
    "            display(HTML(\"<h3>📂 Currently Downloaded Files:</h3>\"))\n",
    "            for file in files:\n",
    "                file_path = os.path.join(download_dir, file)\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                display(HTML(f\"• <code>{file}</code> ({file_size} bytes)\"))\n",
    "        else:\n",
    "            display(HTML(\"<span style='color: orange;'>No files downloaded yet.</span>\"))\n",
    "    else:\n",
    "        display(HTML(\"<span style='color: orange;'>Download directory doesn't exist yet.</span>\"))\n",
    "\n",
    "# %%\n",
    "# Show downloaded files button\n",
    "show_files_button = widgets.Button(\n",
    "    description='📁 Show Downloaded Files',\n",
    "    button_style='info'\n",
    ")\n",
    "\n",
    "def on_show_files_click(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        show_downloaded_files()\n",
    "\n",
    "show_files_button.on_click(on_show_files_click)\n",
    "display(show_files_button)\n",
    "\n",
    "# %%\n",
    "# Cleanup function (optional)\n",
    "def cleanup_downloads():\n",
    "    import shutil\n",
    "    download_dir = \"github_python_journey\"\n",
    "    if os.path.exists(download_dir):\n",
    "        shutil.rmtree(download_dir)\n",
    "        display(HTML(\"<span style='color: green;'>✅ Download directory cleaned up!</span>\"))\n",
    "    else:\n",
    "        display(HTML(\"<span style='color: orange;'>No download directory to clean.</span>\"))\n",
    "\n",
    "# %%\n",
    "# Cleanup button\n",
    "cleanup_button = widgets.Button(\n",
    "    description='🗑️ Cleanup Downloads',\n",
    "    button_style='warning',\n",
    "    tooltip='Delete all downloaded files'\n",
    ")\n",
    "\n",
    "def on_cleanup_click(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        cleanup_downloads()\n",
    "\n",
    "cleanup_button.on_click(on_cleanup_click)\n",
    "display(cleanup_button)\n",
    "\n",
    "# %%\n",
    "# Create a separate output for cleanup/show files\n",
    "output = widgets.Output()\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae237d-0543-4f6c-add1-d673459a1df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
