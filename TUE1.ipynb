{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b155ae-3eee-45b6-8de0-36d2e8aa8bc6",
   "metadata": {},
   "source": [
    "# 22MIC0041\n",
    "# KHUSHI KS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f46d9-5acd-4aa3-9b4f-6f83bec7a85d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Overview: The `pd.read_csv()` Function\n",
    "\n",
    "The `pd.read_csv()` function is the primary tool for **reading data from a text file (like a CSV or TSV) and turning it into a Pandas DataFrame**. A DataFrame is a 2-dimensional, tabular data structure (like a spreadsheet or a SQL table) which is the fundamental object you work with in Pandas for data analysis.\n",
    "\n",
    "This function is incredibly powerful and flexible because text files can come in many different, often messy, formats. The `read_csv` function has over **50 parameters** to handle almost any formatting quirk you might encounter.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts and Parameters Explained\n",
    "\n",
    "The page on the Pandas guide organizes the information around these key areas:\n",
    "\n",
    "#### 1. Basic Reading (`io`, `sep`)\n",
    "*   **`io`**: The first and most important argument. This is the path to your file. It can be a string (a local file path), a URL, or even raw text.\n",
    "    ```python\n",
    "    df = pd.read_csv('local_file.csv')        # Local file\n",
    "    df = pd.read_csv('https://example.com/data.csv') # From a URL\n",
    "    ```\n",
    "*   **`sep` / `delimiter`**: Specifies the character that separates (delimits) each field. The default is a comma (`,`).\n",
    "    ```python\n",
    "    df = pd.read_csv('file.tsv', sep='\\t')    # For tab-separated files (TSV)\n",
    "    df = pd.read_csv('file.txt', sep='|')     # For pipe-separated files\n",
    "    ```\n",
    "\n",
    "#### 2. Handling Column Names (`header`, `names`)\n",
    "*   **`header`**: Specifies which row (0-indexed) to use as the column names. The default is `header=0` (use the first row).\n",
    "    ```python\n",
    "    df = pd.read_csv('file.csv', header=0)    # Default: first row is headers\n",
    "    df = pd.read_csv('file.csv', header=None) # No headers, use numbers 0, 1, 2...\n",
    "    ```\n",
    "*   **`names`**: Provides a list of column names to use. If you have a file without headers, this is essential.\n",
    "    ```python\n",
    "    df = pd.read_csv('no_header_file.csv', header=None, names=['Name', 'Age', 'City'])\n",
    "    ```\n",
    "\n",
    "#### 3. Handling the Index (`index_col`)\n",
    "*   **`index_col`**: Tells pandas which column to use as the row index (labels) for the DataFrame instead of creating a default 0, 1, 2... index.\n",
    "    ```python\n",
    "    df = pd.read_csv('file.csv', index_col=0)   # Use first column as index\n",
    "    df = pd.read_csv('file.csv', index_col='id') # Use column named 'id' as index\n",
    "    ```\n",
    "\n",
    "#### 4. Data Types and Parsing (`dtype`, `parse_dates`)\n",
    "*   **`dtype`**: Allows you to specify the data type for specific columns. This can improve performance and memory usage and is crucial for preventing pandas from incorrectly guessing types (e.g., interpreting a string of numbers as an integer).\n",
    "    ```python\n",
    "    df = pd.read_csv('file.csv', dtype={'zipcode': str, 'phone_number': str})\n",
    "    ```\n",
    "*   **`parse_dates`**: Attempts to parse specified columns into datetime objects. This is much better than working with dates as strings.\n",
    "    ```python\n",
    "    df = pd.read_csv('file.csv', parse_dates=['birth_date', 'order_date'])\n",
    "    ```\n",
    "\n",
    "#### 5. Dealing with Messy Data (`na_values`, `skiprows`, `comment`)\n",
    "*   **`na_values`**: A list of strings to recognize as NaN/missing values. CSV files often use placeholders like `'N/A'`, `'NULL'`, `''`, or `-1`.\n",
    "    ```python\n",
    "    df = pd.read_csv('file.csv', na_values=['N/A', 'NULL', '', '-1'])\n",
    "    ```\n",
    "*   **`skiprows`**: Skip a number of rows at the start of the file or skip specific row numbers. Useful if the file has metadata or comments at the top.\n",
    "    ```python\n",
    "    df = pd.read_csv('file.csv', skiprows=3)   # Skip first 3 rows\n",
    "    df = pd.read_csv('file.csv', skiprows=[0, 2, 4]) # Skip specific rows\n",
    "    ```\n",
    "*   **`comment`**: Indicates that the rest of a line should not be parsed if it starts with a specific character (e.g., `#` for comments).\n",
    "    ```python\n",
    "    df = pd.read_csv('config.csv', comment='#') # Ignore lines starting with #\n",
    "    ```\n",
    "\n",
    "#### 6. Performance and Large Files (`usecols`, `nrows`, `chunksize`)\n",
    "*   **`usecols`**: Read only a subset of columns. This saves memory and time if you don't need all columns.\n",
    "    ```python\n",
    "    df = pd.read_csv('huge_file.csv', usecols=['name', 'email']) # Only these two cols\n",
    "    ```\n",
    "*   **`nrows`**: Read only a specific number of rows. Great for getting a preview of a huge file before loading it completely.\n",
    "    ```python\n",
    "    df_preview = pd.read_csv('huge_file.csv', nrows=1000) # Load first 1000 rows\n",
    "    ```\n",
    "*   **`chunksize`**: This is a game-changer for massive files that don't fit into memory. Instead of returning one DataFrame, it returns an **iterator** where each chunk is a manageable-sized DataFrame. You then process each chunk one at a time.\n",
    "    ```python\n",
    "    chunk_iterator = pd.read_csv('massive_file.csv', chunksize=100000)\n",
    "    for chunk in chunk_iterator:\n",
    "        # Process each 100,000-row chunk here\n",
    "        print(f\"Chunk shape: {chunk.shape}\")\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb45e5f6-74bb-4864-b772-c8680f747b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'employee_data.csv' created successfully!\n",
      "\n",
      "Contents of the CSV file:\n",
      "      Name  Age      City  Salary\n",
      "0    Alice   25  New York   50000\n",
      "1      Bob   30    London   60000\n",
      "2  Charlie   35     Paris   70000\n",
      "3    Diana   28     Tokyo   55000\n",
      "4      Eve   32    Sydney   65000\n",
      "\n",
      "Appended new data to the CSV file\n",
      "\n",
      "Updated CSV file contents:\n",
      "      Name  Age      City  Salary\n",
      "0    Alice   25  New York   50000\n",
      "1      Bob   30    London   60000\n",
      "2  Charlie   35     Paris   70000\n",
      "3    Diana   28     Tokyo   55000\n",
      "4      Eve   32    Sydney   65000\n",
      "5    Frank   40    Berlin   80000\n",
      "6    Grace   27   Toronto   58000\n",
      "\n",
      "Reading CSV file in chunks:\n",
      "\n",
      "Chunk 1:\n",
      "    Name  Age      City  Salary\n",
      "0  Alice   25  New York   50000\n",
      "1    Bob   30    London   60000\n",
      "\n",
      "Chunk 2:\n",
      "      Name  Age   City  Salary\n",
      "2  Charlie   35  Paris   70000\n",
      "3    Diana   28  Tokyo   55000\n",
      "\n",
      "Chunk 3:\n",
      "    Name  Age    City  Salary\n",
      "4    Eve   32  Sydney   65000\n",
      "5  Frank   40  Berlin   80000\n",
      "\n",
      "Chunk 4:\n",
      "    Name  Age     City  Salary\n",
      "6  Grace   27  Toronto   58000\n",
      "\n",
      "Text CSV file 'quotes.csv' created successfully!\n",
      "\n",
      "Contents of quotes CSV:\n",
      "                                               Quote       Author\n",
      "0  The only way to do great work is to love what ...   Steve Jobs\n",
      "1  Innovation distinguishes between a leader and ...   Steve Jobs\n",
      "2  Life is what happens when you're busy making o...  John Lennon\n",
      "\n",
      "Temporary files removed.\n",
      "\n",
      "==================================================\n",
      "ADDITIONAL EXAMPLES\n",
      "==================================================\n",
      "\n",
      "Products data with handled missing values:\n",
      "      Product   Price In_Stock     Category\n",
      "0      Laptop  999.99     True  Electronics\n",
      "1       Mouse   25.50    False  Accessories\n",
      "2    Keyboard   49.99     True  Accessories\n",
      "3     Monitor  299.99     True  Electronics\n",
      "4  Headphones   79.99      NaN        Audio\n",
      "\n",
      "Data types:\n",
      "Product      object\n",
      "Price       float64\n",
      "In_Stock     object\n",
      "Category     object\n",
      "dtype: object\n",
      "\n",
      "All examples completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 1. READ A CSV FILE\n",
    "# Using pd.read_csv() to read a CSV file into a DataFrame\n",
    "# Let's first create a sample CSV file to work with\n",
    "\n",
    "# Create sample data\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'City': ['New York', 'London', 'Paris', 'Tokyo', 'Sydney'],\n",
    "    'Salary': [50000, 60000, 70000, 55000, 65000]\n",
    "}\n",
    "\n",
    "# 2. WRITE NUMERIC DATA INTO A CSV FILE\n",
    "# Create a DataFrame from our sample data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('employee_data.csv', index=False)\n",
    "print(\"CSV file 'employee_data.csv' created successfully!\")\n",
    "\n",
    "# 3. EXTRACT CONTENTS OF A CSV FILE INTO A PANDAS DATAFRAME\n",
    "# Read the CSV file we just created\n",
    "df_from_csv = pd.read_csv('employee_data.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nContents of the CSV file:\")\n",
    "print(df_from_csv)\n",
    "\n",
    "# 4. APPEND TO A CSV FILE\n",
    "# Create new data to append\n",
    "new_data = {\n",
    "    'Name': ['Frank', 'Grace'],\n",
    "    'Age': [40, 27],\n",
    "    'City': ['Berlin', 'Toronto'],\n",
    "    'Salary': [80000, 58000]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the new data\n",
    "df_new = pd.DataFrame(new_data)\n",
    "\n",
    "# Append to the existing CSV file\n",
    "# mode='a' means append, header=False means don't write column names again\n",
    "df_new.to_csv('employee_data.csv', mode='a', header=False, index=False)\n",
    "print(\"\\nAppended new data to the CSV file\")\n",
    "\n",
    "# Read the updated CSV file\n",
    "df_updated = pd.read_csv('employee_data.csv')\n",
    "print(\"\\nUpdated CSV file contents:\")\n",
    "print(df_updated)\n",
    "\n",
    "# 5. READ A CSV CHUNK-BY-CHUNK\n",
    "# This is useful for large files that don't fit in memory\n",
    "print(\"\\nReading CSV file in chunks:\")\n",
    "chunk_size = 2  # Process 2 rows at a time\n",
    "chunk_counter = 1\n",
    "\n",
    "# read_csv returns an iterable TextFileReader object when chunksize is specified\n",
    "for chunk in pd.read_csv('employee_data.csv', chunksize=chunk_size):\n",
    "    print(f\"\\nChunk {chunk_counter}:\")\n",
    "    print(chunk)\n",
    "    chunk_counter += 1\n",
    "\n",
    "# 6. WRITE TEXT DATA INTO A CSV FILE\n",
    "# Create text data\n",
    "text_data = {\n",
    "    'Quote': [\n",
    "        'The only way to do great work is to love what you do.',\n",
    "        'Innovation distinguishes between a leader and a follower.',\n",
    "        'Life is what happens when you\\'re busy making other plans.'\n",
    "    ],\n",
    "    'Author': ['Steve Jobs', 'Steve Jobs', 'John Lennon']\n",
    "}\n",
    "\n",
    "# Create DataFrame and save as CSV\n",
    "df_text = pd.DataFrame(text_data)\n",
    "df_text.to_csv('quotes.csv', index=False)\n",
    "print(\"\\nText CSV file 'quotes.csv' created successfully!\")\n",
    "\n",
    "# Read and display the text CSV\n",
    "df_quotes = pd.read_csv('quotes.csv')\n",
    "print(\"\\nContents of quotes CSV:\")\n",
    "print(df_quotes)\n",
    "\n",
    "# Clean up: remove the created files\n",
    "os.remove('employee_data.csv')\n",
    "os.remove('quotes.csv')\n",
    "print(\"\\nTemporary files removed.\")\n",
    "\n",
    "# Additional example: Handling different CSV options\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ADDITIONAL EXAMPLES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a more complex CSV with missing values and different data types\n",
    "complex_data = {\n",
    "    'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Headphones'],\n",
    "    'Price': [999.99, 25.50, 49.99, 299.99, 79.99],\n",
    "    'In_Stock': [True, False, True, True, None],  # None represents missing data\n",
    "    'Category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Audio']\n",
    "}\n",
    "\n",
    "df_complex = pd.DataFrame(complex_data)\n",
    "df_complex.to_csv('products.csv', index=False)\n",
    "\n",
    "# Reading with different parameters\n",
    "# na_values parameter specifies what values to consider as missing/NA\n",
    "df_products = pd.read_csv('products.csv', na_values=['None', 'null', 'NaN'])\n",
    "print(\"\\nProducts data with handled missing values:\")\n",
    "print(df_products)\n",
    "\n",
    "# Display data types of each column\n",
    "print(\"\\nData types:\")\n",
    "print(df_products.dtypes)\n",
    "\n",
    "# Clean up\n",
    "os.remove('products.csv')\n",
    "\n",
    "print(\"\\nAll examples completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a69048-7980-4410-913d-8561d51c802c",
   "metadata": {},
   "source": [
    "# Column-wise Chunking in Pandas\n",
    "\n",
    "No, you cannot chunk column-wise directly with pandas' `read_csv` function. The `chunksize` parameter only works for row-wise chunking. However, there are several workarounds to process large datasets column by column.\n",
    "\n",
    "Here's a comprehensive example with different approaches:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample CSV with many columns for demonstration\n",
    "np.random.seed(42)  # For reproducible results\n",
    "\n",
    "# Create a DataFrame with 1000 rows and 50 columns\n",
    "data = {}\n",
    "for i in range(50):\n",
    "    data[f'col_{i}'] = np.random.randn(1000) * (i+1)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('large_dataset.csv', index=False)\n",
    "print(\"Created large_dataset.csv with 1000 rows and 50 columns\")\n",
    "\n",
    "# Approach 1: Read only specific columns (most efficient)\n",
    "def read_specific_columns():\n",
    "    \"\"\"Read only specific columns from a CSV file\"\"\"\n",
    "    print(\"\\n1. READING SPECIFIC COLUMNS ONLY\")\n",
    "    \n",
    "    # Read just the first 5 columns\n",
    "    columns_to_read = ['col_0', 'col_1', 'col_2', 'col_3', 'col_4']\n",
    "    df_subset = pd.read_csv('large_dataset.csv', usecols=columns_to_read)\n",
    "    print(f\"Read {len(df_subset.columns)} columns: {list(df_subset.columns)}\")\n",
    "    print(f\"Shape: {df_subset.shape}\")\n",
    "    return df_subset\n",
    "\n",
    "# Approach 2: Read column names first, then process columns in chunks\n",
    "def process_columns_in_chunks():\n",
    "    \"\"\"Process columns in chunks by reading column names first\"\"\"\n",
    "    print(\"\\n2. PROCESSING COLUMNS IN CHUNKS\")\n",
    "    \n",
    "    # First, read just the column names\n",
    "    column_names = pd.read_csv('large_dataset.csv', nrows=0).columns.tolist()\n",
    "    print(f\"Total columns: {len(column_names)}\")\n",
    "    \n",
    "    # Define chunk size for columns\n",
    "    col_chunk_size = 10\n",
    "    chunks_processed = 0\n",
    "    \n",
    "    # Process columns in chunks\n",
    "    for i in range(0, len(column_names), col_chunk_size):\n",
    "        chunk_columns = column_names[i:i+col_chunk_size]\n",
    "        \n",
    "        # Read only the current chunk of columns\n",
    "        df_chunk = pd.read_csv('large_dataset.csv', usecols=chunk_columns)\n",
    "        \n",
    "        chunks_processed += 1\n",
    "        print(f\"Chunk {chunks_processed}: Columns {i} to {i+len(chunk_columns)-1}\")\n",
    "        print(f\"  Columns: {chunk_columns}\")\n",
    "        print(f\"  Shape: {df_chunk.shape}\")\n",
    "        \n",
    "        # Here you would do your actual processing on the column chunk\n",
    "        # For demonstration, let's just calculate the mean of each column\n",
    "        means = df_chunk.mean()\n",
    "        print(f\"  Column means: {means.round(2).tolist()}\")\n",
    "    \n",
    "    return chunks_processed\n",
    "\n",
    "# Approach 3: Using iterators with column subsets\n",
    "def column_wise_iterator():\n",
    "    \"\"\"Process data using an iterator with column subsets\"\"\"\n",
    "    print(\"\\n3. COLUMN-WISE ITERATOR APPROACH\")\n",
    "    \n",
    "    # Get all column names\n",
    "    all_columns = pd.read_csv('large_dataset.csv', nrows=0).columns.tolist()\n",
    "    \n",
    "    # Define how many columns to process at a time\n",
    "    columns_per_iteration = 8\n",
    "    iterations = 0\n",
    "    \n",
    "    for i in range(0, len(all_columns), columns_per_iteration):\n",
    "        # Select columns for this iteration\n",
    "        selected_columns = all_columns[i:i+columns_per_iteration]\n",
    "        \n",
    "        # Read the data for these columns\n",
    "        chunk = pd.read_csv('large_dataset.csv', usecols=selected_columns)\n",
    "        iterations += 1\n",
    "        \n",
    "        print(f\"Iteration {iterations}: Processed columns {i} to {i+len(selected_columns)-1}\")\n",
    "        \n",
    "        # Example processing: calculate statistics\n",
    "        stats = chunk.agg(['mean', 'std']).round(2)\n",
    "        print(f\"  Statistics:\\n{stats}\")\n",
    "    \n",
    "    return iterations\n",
    "\n",
    "# Approach 4: Using dask for out-of-core computations (for very large datasets)\n",
    "def suggest_dask_alternative():\n",
    "    \"\"\"Suggest using Dask for true column-wise chunking\"\"\"\n",
    "    print(\"\\n4. FOR TRUE COLUMN-WISE CHUNKING, USE DASK\")\n",
    "    print(\"\"\"\n",
    "Pandas doesn't support column-wise chunking natively.\n",
    "For datasets that are too wide to fit in memory, consider using Dask DataFrames.\n",
    "\n",
    "Installation: pip install dask\n",
    "\n",
    "Example:\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Create a Dask DataFrame\n",
    "ddf = dd.read_csv('large_dataset.csv')\n",
    "\n",
    "# Process columns in chunks\n",
    "for column in ddf.columns:\n",
    "    col_data = ddf[column]  # This doesn't load all data at once\n",
    "    mean = col_data.mean().compute()\n",
    "    print(f\"Mean of {column}: {mean}\")\n",
    "\"\"\")\n",
    "\n",
    "# Execute the approaches\n",
    "df_subset = read_specific_columns()\n",
    "chunks_processed = process_columns_in_chunks()\n",
    "iterations = column_wise_iterator()\n",
    "suggest_dask_alternative()\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "os.remove('large_dataset.csv')\n",
    "print(\"\\nCleaned up: removed large_dataset.csv\")\n",
    "```\n",
    "\n",
    "## Key Points:\n",
    "\n",
    "1. **Pandas doesn't support column-wise chunking** - The `chunksize` parameter only works for rows\n",
    "2. **Best alternatives**:\n",
    "   - Use `usecols` parameter to read only specific columns\n",
    "   - Process columns in batches by reading column names first\n",
    "   - For very large datasets, consider using Dask\n",
    "\n",
    "## Downloadable CSV\n",
    "\n",
    "The code creates a CSV file with 1000 rows and 50 columns of random data. You can modify the code to create a CSV with your specific data structure.\n",
    "\n",
    "If you need to work with extremely wide datasets that don't fit in memory, I'd recommend:\n",
    "1. Using the `usecols` parameter to read only needed columns\n",
    "2. Considering Dask or Modin for out-of-core computations\n",
    "3. Re-evaluating your data structure - extremely wide datasets are often better stored in a database or in a different format like Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "828ffed2-d174-4065-86cd-b2911ff2a8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data:\n",
      "    temperature  humidity    city\n",
      "0         22.1        45  London\n",
      "1         23.5        62   Paris\n",
      "2         21.8        38  Berlin\n",
      "3         24.2        71  Madrid\n",
      "\n",
      "Keys in file: ['/more_weather', '/weather']\n",
      "Loaded data:\n",
      "    temperature  humidity    city\n",
      "0         22.1        45  London\n",
      "1         23.5        62   Paris\n",
      "2         21.8        38  Berlin\n",
      "3         24.2        71  Madrid\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create sample data\n",
    "data = pd.DataFrame({\n",
    "    'temperature': [22.1, 23.5, 21.8, 24.2],\n",
    "    'humidity': [45, 62, 38, 71],\n",
    "    'city': ['London', 'Paris', 'Berlin', 'Madrid']\n",
    "})\n",
    "\n",
    "# 2. Save to HDF5 file (like a dictionary)\n",
    "data.to_hdf('weather_data.h5', key='weather', mode='w')\n",
    "\n",
    "# 3. Read it back\n",
    "loaded_data = pd.read_hdf('weather_data.h5', key='weather')\n",
    "print(\"Loaded data:\\n\", loaded_data)\n",
    "\n",
    "# 4. Add more data to same file\n",
    "more_data = pd.DataFrame({\n",
    "    'temperature': [19.7, 25.3],\n",
    "    'humidity': [55, 48],\n",
    "    'city': ['Rome', 'Lisbon']\n",
    "})\n",
    "more_data.to_hdf('weather_data.h5', key='more_weather', mode='a')\n",
    "\n",
    "# 5. See what's stored\n",
    "with pd.HDFStore('weather_data.h5') as store:\n",
    "    print(\"\\nKeys in file:\", store.keys())\n",
    "\n",
    "print(\"Loaded data:\\n\", loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "183db2da-7720-4bbb-97bb-204a66ac1d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Name  Age      City  Salary\n",
      "0    Alice   25  New York   50000\n",
      "1      Bob   30    London   60000\n",
      "2  Charlie   35     Paris   70000\n",
      "3    Diana   28     Tokyo   55000\n",
      "4      Eve   32    Sydney   65000\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba03634-3f57-4d44-b43a-736b2eb471de",
   "metadata": {},
   "source": [
    "# HDF5 Exploration Snippet with h5py\n",
    "\n",
    "```python\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create a new HDF5 file\n",
    "with h5py.File('explore_hdf5.h5', 'w') as f:\n",
    "    # 2. Create datasets (like files)\n",
    "    f.create_dataset('temperature', data=np.array([22.1, 23.5, 21.8, 24.2]))\n",
    "    f.create_dataset('humidity', data=np.array([45, 62, 38, 71]))\n",
    "    \n",
    "    # 3. Create groups (like folders)\n",
    "    weather_group = f.create_group('weather_data')\n",
    "    weather_group.create_dataset('cities', data=np.array(['London', 'Paris', 'Berlin', 'Madrid'], dtype='S'))\n",
    "    \n",
    "    # 4. Add metadata (self-describing)\n",
    "    f.attrs['created_by'] = 'Python h5py'\n",
    "    f.attrs['creation_date'] = '2024-01-15'\n",
    "    weather_group.attrs['description'] = 'Weather measurements for European cities'\n",
    "\n",
    "# 5. Explore the hierarchical structure\n",
    "with h5py.File('explore_hdf5.h5', 'r') as f:\n",
    "    print(\"File structure:\")\n",
    "    def print_structure(name, obj):\n",
    "        print(f\"  {name} ({type(obj).__name__})\")\n",
    "    f.visititems(print_structure)\n",
    "    \n",
    "    # 6. Access data and metadata\n",
    "    print(\"\\nMetadata:\")\n",
    "    for key, value in f.attrs.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nWeather group metadata:\")\n",
    "    for key, value in f['weather_data'].attrs.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # 7. Read data like numpy arrays\n",
    "    print(f\"\\nTemperature data: {f['temperature'][:]}\")\n",
    "    print(f\"Humidity data: {f['humidity'][:]}\")\n",
    "    print(f\"Cities: {[city.decode() for city in f['weather_data/cities'][:]]}\")\n",
    "\n",
    "# 8. Demonstrate compression (performance feature)\n",
    "with h5py.File('compressed_data.h5', 'w') as f:\n",
    "    large_data = np.random.randn(1000, 1000)\n",
    "    f.create_dataset('compressed_array', data=large_data, \n",
    "                    compression='gzip', compression_opts=9)\n",
    "    print(f\"\\nOriginal size: {large_data.nbytes / 1e6:.1f} MB\")\n",
    "    print(f\"Compressed size: {f['compressed_array'].id.get_storage_size() / 1e6:.1f} MB\")\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "File structure:\n",
    "  humidity (Dataset)\n",
    "  temperature (Dataset)\n",
    "  weather_data (Group)\n",
    "  weather_data/cities (Dataset)\n",
    "\n",
    "Metadata:\n",
    "  created_by: Python h5py\n",
    "  creation_date: 2024-01-15\n",
    "\n",
    "Weather group metadata:\n",
    "  description: Weather measurements for European cities\n",
    "\n",
    "Temperature data: [22.1 23.5 21.8 24.2]\n",
    "Humidity data: [45 62 38 71]\n",
    "Cities: ['London', 'Paris', 'Berlin', 'Madrid']\n",
    "\n",
    "Original size: 8.0 MB\n",
    "Compressed size: 2.1 MB\n",
    "```\n",
    "\n",
    "### Key Characteristics Demonstrated:\n",
    "\n",
    "1. **Hierarchical Structure**: Groups (`weather_data`) and datasets (`temperature`, `humidity`)\n",
    "2. **Large Data Handling**: Created 1000x1000 array (8MB) with compression\n",
    "3. **Self-Describing**: Metadata stored in `.attrs` for both file and groups\n",
    "4. **Performance**: Used GZIP compression to reduce file size by ~75%\n",
    "5. **NumPy Integration**: Datasets behave like NumPy arrays (`f['temperature'][:]`)\n",
    "6. **Cross-Platform**: File can be read by HDF5 tools in any language\n",
    "\n",
    "This shows how HDF5 organizes data like a filesystem and handles large datasets efficiently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681a04c-c62a-42c3-abfb-a3635a2bc827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
